{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f9c5ca614b0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Entropy Loss (Low-level)\n",
    "$$ L = \\frac{1}{N} \\sum - y \\log(\\hat{y}) $$\n",
    "where $\\hat{y}$ is the predicted probability and $y$ is the correct probability (0 or 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2201, 0.1774, 0.2032, 0.1925, 0.2068],\n",
      "        [0.2794, 0.1372, 0.2804, 0.1534, 0.1495],\n",
      "        [0.2203, 0.1962, 0.2866, 0.1599, 0.1370]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([1, 1, 4])\n"
     ]
    }
   ],
   "source": [
    "z = torch.rand(3,5,requires_grad= True)\n",
    "hypothesis = F.softmax(z,dim = 1)\n",
    "print(hypothesis)\n",
    "y = torch.randint(5,(3,)).long()\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_one_hot = torch.zeros_like(hypothesis)\n",
    "#!!!!!\n",
    "y_one_hot.scatter_(1,y.unsqueeze(1),1)  #dim = 1, y.unsqueeze(1): (3,) -> (3,1), 1의 값을 뿌리기 \n",
    "cost = (y_one_hot*(-torch.log(hypothesis))).sum(dim=1).mean()   #차피 1부분만 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# with torch.nn.functional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.5135, -1.7293, -1.5936, -1.6478, -1.5760],\n",
      "        [-1.2752, -1.9861, -1.2714, -1.8746, -1.9003],\n",
      "        [-1.5129, -1.6286, -1.2497, -1.8329, -1.9878]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "tensor(1.9011, grad_fn=<NllLossBackward>)\n",
      "tensor(1.9011, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "#softmax,log\n",
    "print(F.log_softmax(z,dim = 1))\n",
    "#softmax,log + y -> loss\n",
    "print(F.nll_loss(F.log_softmax(z,dim=1),y))\n",
    "#혹은, 간편하게\n",
    "print(F.cross_entropy(z,y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# training with F.cross_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = [[1, 2, 1, 1],\n",
    "           [2, 1, 3, 2],\n",
    "           [3, 1, 3, 4],\n",
    "           [4, 1, 5, 5],\n",
    "           [1, 7, 5, 5],\n",
    "           [1, 2, 5, 6],\n",
    "           [1, 6, 6, 6],\n",
    "           [1, 7, 7, 7]]\n",
    "y_train = [2, 2, 2, 1, 1, 1, 0, 0]\n",
    "x_train = torch.FloatTensor(x_train)\n",
    "y_train = torch.LongTensor(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:    0/1000 Cost: 1.098612\n",
      "epoch:  100/1000 Cost: 0.761050\n",
      "epoch:  200/1000 Cost: 0.689991\n",
      "epoch:  300/1000 Cost: 0.643229\n",
      "epoch:  400/1000 Cost: 0.604117\n",
      "epoch:  500/1000 Cost: 0.568255\n",
      "epoch:  600/1000 Cost: 0.533922\n",
      "epoch:  700/1000 Cost: 0.500291\n",
      "epoch:  800/1000 Cost: 0.466908\n",
      "epoch:  900/1000 Cost: 0.433507\n",
      "epoch: 1000/1000 Cost: 0.399962\n"
     ]
    }
   ],
   "source": [
    "W = torch.zeros((4,3),requires_grad = True)\n",
    "b = torch.zeros(1, requires_grad = True)\n",
    "\n",
    "optimizer = optim.SGD([W,b], lr = 0.1)\n",
    "epochs = 1000\n",
    "for epoch in range(epochs +1):\n",
    "    z = x_train.matmul(W)+b\n",
    "    cost = F.cross_entropy(z, y_train)\n",
    "    #이 함수 안에 softmax, log, y_onehot을 구하여 loss를 구하는 과정이 전부 들어있다.\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch % 100 ==0:\n",
    "        print('epoch: {:4d}/{} Cost: {:.6f}'.format(epoch, epochs, cost.item()))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# High-level Implementation with nn.Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(4,3) #네개의 input을 받아서 3개의 class에 대한 확률값을 output\n",
    "    def forward(self,x):\n",
    "        return self.linear(x)    # (m,4) -> (m,3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SoftmaxClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:    0/1000 cost: 1.581921\n",
      "epoch:   10/1000 cost: 1.144623\n",
      "epoch:   20/1000 cost: 0.992444\n",
      "epoch:   30/1000 cost: 0.911484\n",
      "epoch:   40/1000 cost: 0.861131\n",
      "epoch:   50/1000 cost: 0.825565\n",
      "epoch:   60/1000 cost: 0.798587\n",
      "epoch:   70/1000 cost: 0.777095\n",
      "epoch:   80/1000 cost: 0.759307\n",
      "epoch:   90/1000 cost: 0.744129\n",
      "epoch:  100/1000 cost: 0.730856\n",
      "epoch:  110/1000 cost: 0.719018\n",
      "epoch:  120/1000 cost: 0.708289\n",
      "epoch:  130/1000 cost: 0.698438\n",
      "epoch:  140/1000 cost: 0.689294\n",
      "epoch:  150/1000 cost: 0.680729\n",
      "epoch:  160/1000 cost: 0.672645\n",
      "epoch:  170/1000 cost: 0.664966\n",
      "epoch:  180/1000 cost: 0.657629\n",
      "epoch:  190/1000 cost: 0.650587\n",
      "epoch:  200/1000 cost: 0.643798\n",
      "epoch:  210/1000 cost: 0.637231\n",
      "epoch:  220/1000 cost: 0.630856\n",
      "epoch:  230/1000 cost: 0.624651\n",
      "epoch:  240/1000 cost: 0.618597\n",
      "epoch:  250/1000 cost: 0.612676\n",
      "epoch:  260/1000 cost: 0.606875\n",
      "epoch:  270/1000 cost: 0.601180\n",
      "epoch:  280/1000 cost: 0.595582\n",
      "epoch:  290/1000 cost: 0.590070\n",
      "epoch:  300/1000 cost: 0.584637\n",
      "epoch:  310/1000 cost: 0.579276\n",
      "epoch:  320/1000 cost: 0.573979\n",
      "epoch:  330/1000 cost: 0.568742\n",
      "epoch:  340/1000 cost: 0.563558\n",
      "epoch:  350/1000 cost: 0.558424\n",
      "epoch:  360/1000 cost: 0.553336\n",
      "epoch:  370/1000 cost: 0.548290\n",
      "epoch:  380/1000 cost: 0.543282\n",
      "epoch:  390/1000 cost: 0.538310\n",
      "epoch:  400/1000 cost: 0.533371\n",
      "epoch:  410/1000 cost: 0.528461\n",
      "epoch:  420/1000 cost: 0.523581\n",
      "epoch:  430/1000 cost: 0.518726\n",
      "epoch:  440/1000 cost: 0.513895\n",
      "epoch:  450/1000 cost: 0.509086\n",
      "epoch:  460/1000 cost: 0.504298\n",
      "epoch:  470/1000 cost: 0.499529\n",
      "epoch:  480/1000 cost: 0.494779\n",
      "epoch:  490/1000 cost: 0.490044\n",
      "epoch:  500/1000 cost: 0.485325\n",
      "epoch:  510/1000 cost: 0.480621\n",
      "epoch:  520/1000 cost: 0.475929\n",
      "epoch:  530/1000 cost: 0.471249\n",
      "epoch:  540/1000 cost: 0.466580\n",
      "epoch:  550/1000 cost: 0.461922\n",
      "epoch:  560/1000 cost: 0.457274\n",
      "epoch:  570/1000 cost: 0.452634\n",
      "epoch:  580/1000 cost: 0.448002\n",
      "epoch:  590/1000 cost: 0.443378\n",
      "epoch:  600/1000 cost: 0.438760\n",
      "epoch:  610/1000 cost: 0.434148\n",
      "epoch:  620/1000 cost: 0.429542\n",
      "epoch:  630/1000 cost: 0.424941\n",
      "epoch:  640/1000 cost: 0.420345\n",
      "epoch:  650/1000 cost: 0.415753\n",
      "epoch:  660/1000 cost: 0.411164\n",
      "epoch:  670/1000 cost: 0.406579\n",
      "epoch:  680/1000 cost: 0.401997\n",
      "epoch:  690/1000 cost: 0.397417\n",
      "epoch:  700/1000 cost: 0.392840\n",
      "epoch:  710/1000 cost: 0.388265\n",
      "epoch:  720/1000 cost: 0.383692\n",
      "epoch:  730/1000 cost: 0.379120\n",
      "epoch:  740/1000 cost: 0.374551\n",
      "epoch:  750/1000 cost: 0.369983\n",
      "epoch:  760/1000 cost: 0.365417\n",
      "epoch:  770/1000 cost: 0.360852\n",
      "epoch:  780/1000 cost: 0.356290\n",
      "epoch:  790/1000 cost: 0.351730\n",
      "epoch:  800/1000 cost: 0.347172\n",
      "epoch:  810/1000 cost: 0.342618\n",
      "epoch:  820/1000 cost: 0.338068\n",
      "epoch:  830/1000 cost: 0.333523\n",
      "epoch:  840/1000 cost: 0.328983\n",
      "epoch:  850/1000 cost: 0.324450\n",
      "epoch:  860/1000 cost: 0.319926\n",
      "epoch:  870/1000 cost: 0.315413\n",
      "epoch:  880/1000 cost: 0.310914\n",
      "epoch:  890/1000 cost: 0.306430\n",
      "epoch:  900/1000 cost: 0.301966\n",
      "epoch:  910/1000 cost: 0.297527\n",
      "epoch:  920/1000 cost: 0.293118\n",
      "epoch:  930/1000 cost: 0.288747\n",
      "epoch:  940/1000 cost: 0.284423\n",
      "epoch:  950/1000 cost: 0.280157\n",
      "epoch:  960/1000 cost: 0.275963\n",
      "epoch:  970/1000 cost: 0.271860\n",
      "epoch:  980/1000 cost: 0.267871\n",
      "epoch:  990/1000 cost: 0.264021\n",
      "epoch: 1000/1000 cost: 0.260346\n"
     ]
    }
   ],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr = 0.1)\n",
    "epochs = 1000\n",
    "\n",
    "for epoch in range(epochs+1):\n",
    "    #H(x)계산\n",
    "    prediction = model(x_train) #(m,4) -> (m,3)\n",
    "    #cos 계산\n",
    "    cost = F.cross_entropy(prediction, y_train)\n",
    "    \n",
    "    #cost로 SGD\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch % 10 ==0:\n",
    "        print('epoch: {:4d}/{} cost: {:.6f}'.format(epoch, epochs, cost.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
