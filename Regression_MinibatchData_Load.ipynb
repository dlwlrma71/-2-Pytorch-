{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fefacb614d0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.4321e-05, grad_fn=<MeanBackward0>) tensor([2.0049], requires_grad=True) tensor([0.9857], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "x_train = torch.FloatTensor([[1], [2], [3], [4]])\n",
    "y_train = torch.FloatTensor([[3], [5], [7], [9]])\n",
    "\n",
    "W = torch.zeros(1, requires_grad=True)\n",
    "b = torch.zeros(1, requires_grad=True)\n",
    "#!\n",
    "optimizer = torch.optim.SGD([W, b], lr=0.01)\n",
    "\n",
    "epochs = 1000\n",
    "for epoch in range(1, epochs+1):\n",
    "    h = x_train*W+b\n",
    "    cost = torch.mean((h-y_train)**2)\n",
    "\n",
    "    optimizer.zero_grad()  # * gradient를 0으로 초기화\n",
    "    cost.backward()   # * backpropagation을 통해 gradient계산\n",
    "    optimizer.step()  # * 계산된 방향대로 W,b를 갱신(gradient descent)\n",
    "print(cost, W, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# mutlvariable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 3])\n",
      "tensor([[0.7179],\n",
      "        [0.6126],\n",
      "        [0.6801]], requires_grad=True) tensor([0.0092], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "x_train = torch.FloatTensor([])\n",
    "\n",
    "x1_train = torch.FloatTensor([[73], [93], [89], [96], [73]])\n",
    "x2_train = torch.FloatTensor([[80], [88], [91], [98], [66]])\n",
    "x3_train = torch.FloatTensor([[75], [93], [90], [100], [70]])\n",
    "y_train = torch.FloatTensor([[152], [185], [180], [196], [142]])\n",
    "x_train = torch.cat([x1_train, x2_train, x3_train], dim=1)\n",
    "W = torch.zeros((3, 1), requires_grad=True)\n",
    "b = torch.zeros(1, requires_grad=True)\n",
    "print(x_train.shape)\n",
    "\n",
    "optimizer = optim.SGD([W, b], lr=1e-5)\n",
    "epochs = 1000\n",
    "for i in range(epochs):\n",
    "    h = x_train.matmul(W) + b\n",
    "    cost = torch.mean((h-y_train)**2)\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "print(W, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# nn.Module (high-level implementation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultivariateLinearRegressionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()    # nn.Module(부모 클래스)의 클래스 변수들을 가져올 수 있다.\n",
    "        self.linear = nn.Linear(3,1)    #입력 차원과 출력 차원을 알려줌\n",
    "    def forward(self, x):\n",
    "        result = self.linear(x)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1/200 Cost: 11517.784180\n",
      "epoch: 2/200 Cost: 10526.354492\n",
      "epoch: 3/200 Cost: 9620.277344\n",
      "epoch: 4/200 Cost: 8792.205078\n",
      "epoch: 5/200 Cost: 8035.421875\n",
      "epoch: 6/200 Cost: 7343.790039\n",
      "epoch: 7/200 Cost: 6711.700195\n",
      "epoch: 8/200 Cost: 6134.025879\n",
      "epoch: 9/200 Cost: 5606.085449\n",
      "epoch: 10/200 Cost: 5123.594727\n",
      "epoch: 11/200 Cost: 4682.642578\n",
      "epoch: 12/200 Cost: 4279.651367\n",
      "epoch: 13/200 Cost: 3911.353516\n",
      "epoch: 14/200 Cost: 3574.762939\n",
      "epoch: 15/200 Cost: 3267.149658\n",
      "epoch: 16/200 Cost: 2986.018066\n",
      "epoch: 17/200 Cost: 2729.089355\n",
      "epoch: 18/200 Cost: 2494.279297\n",
      "epoch: 19/200 Cost: 2279.684326\n",
      "epoch: 20/200 Cost: 2083.564209\n",
      "epoch: 21/200 Cost: 1904.327881\n",
      "epoch: 22/200 Cost: 1740.522705\n",
      "epoch: 23/200 Cost: 1590.818359\n",
      "epoch: 24/200 Cost: 1454.002441\n",
      "epoch: 25/200 Cost: 1328.965088\n",
      "epoch: 26/200 Cost: 1214.692871\n",
      "epoch: 27/200 Cost: 1110.257690\n",
      "epoch: 28/200 Cost: 1014.813843\n",
      "epoch: 29/200 Cost: 927.586060\n",
      "epoch: 30/200 Cost: 847.868347\n",
      "epoch: 31/200 Cost: 775.013062\n",
      "epoch: 32/200 Cost: 708.429993\n",
      "epoch: 33/200 Cost: 647.579285\n",
      "epoch: 34/200 Cost: 591.966919\n",
      "epoch: 35/200 Cost: 541.142700\n",
      "epoch: 36/200 Cost: 494.693420\n",
      "epoch: 37/200 Cost: 452.243317\n",
      "epoch: 38/200 Cost: 413.447906\n",
      "epoch: 39/200 Cost: 377.991791\n",
      "epoch: 40/200 Cost: 345.588348\n",
      "epoch: 41/200 Cost: 315.974548\n",
      "epoch: 42/200 Cost: 288.910583\n",
      "epoch: 43/200 Cost: 264.175934\n",
      "epoch: 44/200 Cost: 241.570877\n",
      "epoch: 45/200 Cost: 220.912140\n",
      "epoch: 46/200 Cost: 202.031601\n",
      "epoch: 47/200 Cost: 184.776459\n",
      "epoch: 48/200 Cost: 169.007309\n",
      "epoch: 49/200 Cost: 154.595123\n",
      "epoch: 50/200 Cost: 141.423920\n",
      "epoch: 51/200 Cost: 129.386765\n",
      "epoch: 52/200 Cost: 118.385704\n",
      "epoch: 53/200 Cost: 108.331863\n",
      "epoch: 54/200 Cost: 99.143593\n",
      "epoch: 55/200 Cost: 90.746033\n",
      "epoch: 56/200 Cost: 83.071648\n",
      "epoch: 57/200 Cost: 76.057968\n",
      "epoch: 58/200 Cost: 69.648026\n",
      "epoch: 59/200 Cost: 63.789783\n",
      "epoch: 60/200 Cost: 58.436024\n",
      "epoch: 61/200 Cost: 53.543255\n",
      "epoch: 62/200 Cost: 49.071632\n",
      "epoch: 63/200 Cost: 44.984756\n",
      "epoch: 64/200 Cost: 41.249947\n",
      "epoch: 65/200 Cost: 37.836617\n",
      "epoch: 66/200 Cost: 34.717022\n",
      "epoch: 67/200 Cost: 31.866196\n",
      "epoch: 68/200 Cost: 29.260614\n",
      "epoch: 69/200 Cost: 26.879375\n",
      "epoch: 70/200 Cost: 24.703245\n",
      "epoch: 71/200 Cost: 22.714256\n",
      "epoch: 72/200 Cost: 20.896544\n",
      "epoch: 73/200 Cost: 19.235472\n",
      "epoch: 74/200 Cost: 17.717253\n",
      "epoch: 75/200 Cost: 16.329794\n",
      "epoch: 76/200 Cost: 15.061697\n",
      "epoch: 77/200 Cost: 13.902858\n",
      "epoch: 78/200 Cost: 12.843674\n",
      "epoch: 79/200 Cost: 11.875821\n",
      "epoch: 80/200 Cost: 10.991152\n",
      "epoch: 81/200 Cost: 10.182695\n",
      "epoch: 82/200 Cost: 9.443869\n",
      "epoch: 83/200 Cost: 8.768563\n",
      "epoch: 84/200 Cost: 8.151419\n",
      "epoch: 85/200 Cost: 7.587344\n",
      "epoch: 86/200 Cost: 7.071914\n",
      "epoch: 87/200 Cost: 6.600851\n",
      "epoch: 88/200 Cost: 6.170343\n",
      "epoch: 89/200 Cost: 5.776795\n",
      "epoch: 90/200 Cost: 5.417203\n",
      "epoch: 91/200 Cost: 5.088529\n",
      "epoch: 92/200 Cost: 4.788127\n",
      "epoch: 93/200 Cost: 4.513629\n",
      "epoch: 94/200 Cost: 4.262755\n",
      "epoch: 95/200 Cost: 4.033467\n",
      "epoch: 96/200 Cost: 3.823916\n",
      "epoch: 97/200 Cost: 3.632358\n",
      "epoch: 98/200 Cost: 3.457330\n",
      "epoch: 99/200 Cost: 3.297350\n",
      "epoch: 100/200 Cost: 3.151131\n",
      "epoch: 101/200 Cost: 3.017520\n",
      "epoch: 102/200 Cost: 2.895364\n",
      "epoch: 103/200 Cost: 2.783744\n",
      "epoch: 104/200 Cost: 2.681704\n",
      "epoch: 105/200 Cost: 2.588482\n",
      "epoch: 106/200 Cost: 2.503269\n",
      "epoch: 107/200 Cost: 2.425392\n",
      "epoch: 108/200 Cost: 2.354203\n",
      "epoch: 109/200 Cost: 2.289108\n",
      "epoch: 110/200 Cost: 2.229649\n",
      "epoch: 111/200 Cost: 2.175280\n",
      "epoch: 112/200 Cost: 2.125601\n",
      "epoch: 113/200 Cost: 2.080195\n",
      "epoch: 114/200 Cost: 2.038700\n",
      "epoch: 115/200 Cost: 2.000768\n",
      "epoch: 116/200 Cost: 1.966085\n",
      "epoch: 117/200 Cost: 1.934394\n",
      "epoch: 118/200 Cost: 1.905414\n",
      "epoch: 119/200 Cost: 1.878923\n",
      "epoch: 120/200 Cost: 1.854684\n",
      "epoch: 121/200 Cost: 1.832569\n",
      "epoch: 122/200 Cost: 1.812346\n",
      "epoch: 123/200 Cost: 1.793847\n",
      "epoch: 124/200 Cost: 1.776918\n",
      "epoch: 125/200 Cost: 1.761484\n",
      "epoch: 126/200 Cost: 1.747339\n",
      "epoch: 127/200 Cost: 1.734416\n",
      "epoch: 128/200 Cost: 1.722606\n",
      "epoch: 129/200 Cost: 1.711800\n",
      "epoch: 130/200 Cost: 1.701918\n",
      "epoch: 131/200 Cost: 1.692898\n",
      "epoch: 132/200 Cost: 1.684618\n",
      "epoch: 133/200 Cost: 1.677076\n",
      "epoch: 134/200 Cost: 1.670170\n",
      "epoch: 135/200 Cost: 1.663844\n",
      "epoch: 136/200 Cost: 1.658066\n",
      "epoch: 137/200 Cost: 1.652770\n",
      "epoch: 138/200 Cost: 1.647922\n",
      "epoch: 139/200 Cost: 1.643490\n",
      "epoch: 140/200 Cost: 1.639450\n",
      "epoch: 141/200 Cost: 1.635728\n",
      "epoch: 142/200 Cost: 1.632328\n",
      "epoch: 143/200 Cost: 1.629214\n",
      "epoch: 144/200 Cost: 1.626373\n",
      "epoch: 145/200 Cost: 1.623771\n",
      "epoch: 146/200 Cost: 1.621376\n",
      "epoch: 147/200 Cost: 1.619201\n",
      "epoch: 148/200 Cost: 1.617191\n",
      "epoch: 149/200 Cost: 1.615341\n",
      "epoch: 150/200 Cost: 1.613656\n",
      "epoch: 151/200 Cost: 1.612118\n",
      "epoch: 152/200 Cost: 1.610701\n",
      "epoch: 153/200 Cost: 1.609397\n",
      "epoch: 154/200 Cost: 1.608189\n",
      "epoch: 155/200 Cost: 1.607100\n",
      "epoch: 156/200 Cost: 1.606087\n",
      "epoch: 157/200 Cost: 1.605161\n",
      "epoch: 158/200 Cost: 1.604301\n",
      "epoch: 159/200 Cost: 1.603518\n",
      "epoch: 160/200 Cost: 1.602790\n",
      "epoch: 161/200 Cost: 1.602136\n",
      "epoch: 162/200 Cost: 1.601511\n",
      "epoch: 163/200 Cost: 1.600950\n",
      "epoch: 164/200 Cost: 1.600424\n",
      "epoch: 165/200 Cost: 1.599939\n",
      "epoch: 166/200 Cost: 1.599490\n",
      "epoch: 167/200 Cost: 1.599079\n",
      "epoch: 168/200 Cost: 1.598686\n",
      "epoch: 169/200 Cost: 1.598325\n",
      "epoch: 170/200 Cost: 1.598005\n",
      "epoch: 171/200 Cost: 1.597690\n",
      "epoch: 172/200 Cost: 1.597409\n",
      "epoch: 173/200 Cost: 1.597145\n",
      "epoch: 174/200 Cost: 1.596883\n",
      "epoch: 175/200 Cost: 1.596655\n",
      "epoch: 176/200 Cost: 1.596421\n",
      "epoch: 177/200 Cost: 1.596225\n",
      "epoch: 178/200 Cost: 1.596042\n",
      "epoch: 179/200 Cost: 1.595855\n",
      "epoch: 180/200 Cost: 1.595682\n",
      "epoch: 181/200 Cost: 1.595517\n",
      "epoch: 182/200 Cost: 1.595355\n",
      "epoch: 183/200 Cost: 1.595217\n",
      "epoch: 184/200 Cost: 1.595069\n",
      "epoch: 185/200 Cost: 1.594939\n",
      "epoch: 186/200 Cost: 1.594806\n",
      "epoch: 187/200 Cost: 1.594682\n",
      "epoch: 188/200 Cost: 1.594576\n",
      "epoch: 189/200 Cost: 1.594457\n",
      "epoch: 190/200 Cost: 1.594343\n",
      "epoch: 191/200 Cost: 1.594240\n",
      "epoch: 192/200 Cost: 1.594133\n",
      "epoch: 193/200 Cost: 1.594037\n",
      "epoch: 194/200 Cost: 1.593953\n",
      "epoch: 195/200 Cost: 1.593848\n",
      "epoch: 196/200 Cost: 1.593768\n",
      "epoch: 197/200 Cost: 1.593665\n",
      "epoch: 198/200 Cost: 1.593587\n",
      "epoch: 199/200 Cost: 1.593500\n",
      "epoch: 200/200 Cost: 1.593429\n"
     ]
    }
   ],
   "source": [
    "model = MultivariateLinearRegressionModel() #모델 초기화 (W,b따로 정의해줄 필요 없이)\n",
    "optimizer = optim.SGD(model.parameters(),lr = 1e-6)\n",
    "n_epochs = 200\n",
    "for i in range(n_epochs):\n",
    "    prediction = model(x_train)\n",
    "    cost = F.mse_loss(prediction, y_train)\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "    print('epoch: {:1d}/{} Cost: {:.6f}'.format(i+1, n_epochs, cost.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATALOAD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://pytorch.org/tutorials/beginner/data_loading_tutorial.html#iterating-through-the-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataloader.DataLoader object at 0x7fefadb9b610>\n"
     ]
    }
   ],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self,csv_file = None):\n",
    "        #데이터 로드(pandas로 csv파일 불러오는 등)\n",
    "        self.x_data = [[73,80,75],[93,88,93],[96,98,100],[73,66,70]]\n",
    "        self.y_data = [[152],[185],[196],[142]]\n",
    "    def __len__(self):\n",
    "        return len(self.x_data)\n",
    "    def __getitem__(self,idx):\n",
    "        x = torch.FloatTensor(self.x_data[idx])\n",
    "        y = torch.FloatTensor(self.y_data[idx])\n",
    "        return x,y\n",
    "dataset = CustomDataset() \n",
    "dataloader = DataLoader(dataset,batch_size = 2, shuffle = True)\n",
    "print(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch    0/20 Batch1/2 Cost: 3.504112\n",
      "epoch    0/20 Batch2/2 Cost: 0.503981\n",
      "epoch    1/20 Batch1/2 Cost: 0.519529\n",
      "epoch    1/20 Batch2/2 Cost: 3.505044\n",
      "epoch    2/20 Batch1/2 Cost: 3.448957\n",
      "epoch    2/20 Batch2/2 Cost: 0.575886\n",
      "epoch    3/20 Batch1/2 Cost: 0.466791\n",
      "epoch    3/20 Batch2/2 Cost: 3.540088\n",
      "epoch    4/20 Batch1/2 Cost: 0.715349\n",
      "epoch    4/20 Batch2/2 Cost: 3.312751\n",
      "epoch    5/20 Batch1/2 Cost: 0.525308\n",
      "epoch    5/20 Batch2/2 Cost: 3.497788\n",
      "epoch    6/20 Batch1/2 Cost: 3.496427\n",
      "epoch    6/20 Batch2/2 Cost: 0.509208\n",
      "epoch    7/20 Batch1/2 Cost: 3.250491\n",
      "epoch    7/20 Batch2/2 Cost: 0.776163\n",
      "epoch    8/20 Batch1/2 Cost: 0.523577\n",
      "epoch    8/20 Batch2/2 Cost: 3.498270\n",
      "epoch    9/20 Batch1/2 Cost: 3.442507\n",
      "epoch    9/20 Batch2/2 Cost: 0.579620\n",
      "epoch   10/20 Batch1/2 Cost: 0.523065\n",
      "epoch   10/20 Batch2/2 Cost: 3.497982\n",
      "epoch   11/20 Batch1/2 Cost: 3.496222\n",
      "epoch   11/20 Batch2/2 Cost: 0.507411\n",
      "epoch   12/20 Batch1/2 Cost: 0.469784\n",
      "epoch   12/20 Batch2/2 Cost: 3.533459\n",
      "epoch   13/20 Batch1/2 Cost: 3.494984\n",
      "epoch   13/20 Batch2/2 Cost: 0.507872\n",
      "epoch   14/20 Batch1/2 Cost: 0.716463\n",
      "epoch   14/20 Batch2/2 Cost: 3.307562\n",
      "epoch   15/20 Batch1/2 Cost: 3.438812\n",
      "epoch   15/20 Batch2/2 Cost: 0.580963\n",
      "epoch   16/20 Batch1/2 Cost: 3.248988\n",
      "epoch   16/20 Batch2/2 Cost: 0.774043\n",
      "epoch   17/20 Batch1/2 Cost: 0.522444\n",
      "epoch   17/20 Batch2/2 Cost: 3.495841\n",
      "epoch   18/20 Batch1/2 Cost: 3.494149\n",
      "epoch   18/20 Batch2/2 Cost: 0.506751\n",
      "epoch   19/20 Batch1/2 Cost: 0.522985\n",
      "epoch   19/20 Batch2/2 Cost: 3.494497\n",
      "epoch   20/20 Batch1/2 Cost: 0.525097\n",
      "epoch   20/20 Batch2/2 Cost: 3.492042\n"
     ]
    }
   ],
   "source": [
    "np_epochs = 20\n",
    "for epoch in range(np_epochs+1):\n",
    "    for batch_idx, samples in enumerate(dataloader):\n",
    "        x_train,y_train = samples\n",
    "        prediction = model(x_train)\n",
    "        cost = F.mse_loss(prediction, y_train)\n",
    "        optimizer.zero_grad()\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "        print(\"epoch {:4d}/{} Batch{}/{} Cost: {:.6f}\".format(epoch, np_epochs, batch_idx+1, len(dataloader),cost.item()))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
